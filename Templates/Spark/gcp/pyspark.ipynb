{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Migrating from Spark to BigQuery via Dataproc -- Part 1\n\n* [Part 1](01_spark.ipynb): The original Spark code, now running on Dataproc (lift-and-shift).\n* [Part 2](02_gcs.ipynb): Replace HDFS by Google Cloud Storage. This enables job-specific-clusters. (cloud-native)\n* [Part 3](03_automate.ipynb): Automate everything, so that we can run in a job-specific cluster. (cloud-optimized)\n* [Part 4](04_bigquery.ipynb): Load CSV into BigQuery, use BigQuery. (modernize)\n* [Part 5](05_functions.ipynb): Using Cloud Functions, launch analysis every time there is a new file in the bucket. (serverless)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### Copy data to HDFS\n\nThe Spark code in this notebook is based loosely on the [code](https://github.com/dipanjanS/data_science_for_all/blob/master/tds_spark_sql_intro/Working%20with%20SQL%20at%20Scale%20-%20Spark%20SQL%20Tutorial.ipynb) accompanying [this post](https://opensource.com/article/19/3/apache-spark-and-dataframes-tutorial) by Dipanjan Sarkar. I am using it to illustrate migrating a Spark analytics workload to BigQuery via Dataproc.\n\nThe data itself comes from the 1999 KDD competition. Let's grab 10% of the data to use as an illustration."}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Overwriting spark_analysis.py\n"}], "source": "%%writefile spark_analysis.py\n\nimport matplotlib\nmatplotlib.use('agg')\n\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--bucket\", help=\"bucket for input and output\")\nargs = parser.parse_args()\n\nBUCKET = args.bucket"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "--2020-08-12 04:11:10--  http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\nResolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.86\nConnecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.86|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2144903 (2.0M) [application/x-gzip]\nSaving to: \u2018kddcup.data_10_percent.gz\u2019\n\nkddcup.data_10_perc 100%[===================>]   2.04M  5.62MB/s    in 0.4s    \n\n2020-08-12 04:11:11 (5.62 MB/s) - \u2018kddcup.data_10_percent.gz\u2019 saved [2144903/2144903]\n\n"}], "source": "!wget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "!hadoop fs -put kddcup* /"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 4 items\ndrwx------   - mapred hadoop          0 2020-08-12 04:04 /hadoop\n-rw-r--r--   2 root   hadoop    2144903 2020-08-12 04:11 /kddcup.data_10_percent.gz\ndrwxrwxrwt   - hdfs   hadoop          0 2020-08-12 04:04 /tmp\ndrwxrwxrwt   - hdfs   hadoop          0 2020-08-12 04:04 /user\n"}], "source": "!hadoop fs -ls /"}, {"cell_type": "markdown", "metadata": {}, "source": "### Reading in data\n\nThe data are CSV files. In Spark, these can be read using textFile and splitting rows on commas."}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nfrom pyspark.sql import SparkSession, SQLContext, Row\n\ngcs_bucket='qwiklabs-gcp-01-cd84fb191978'\nspark = SparkSession.builder.appName(\"kdd\").getOrCreate()\nsc = spark.sparkContext\ndata_file = \"gs://\"+gcs_bucket+\"//kddcup.data_10_percent.gz\"\nraw_rdd = sc.textFile(data_file).cache()\nraw_rdd.take(5)"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql import SparkSession, SQLContext, Row\n\nspark = SparkSession.builder.appName(\"kdd\").getOrCreate()\nsc = spark.sparkContext\ndata_file = \"hdfs:///kddcup.data_10_percent.gz\"\nraw_rdd = sc.textFile(data_file).cache()\nraw_rdd.take(5)"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\ncsv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\nparsed_rdd = csv_rdd.map(lambda r: Row(\n    duration=int(r[0]), \n    protocol_type=r[1],\n    service=r[2],\n    flag=r[3],\n    src_bytes=int(r[4]),\n    dst_bytes=int(r[5]),\n    wrong_fragment=int(r[7]),\n    urgent=int(r[8]),\n    hot=int(r[9]),\n    num_failed_logins=int(r[10]),\n    num_compromised=int(r[12]),\n    su_attempted=r[14],\n    num_root=int(r[15]),\n    num_file_creations=int(r[16]),\n    label=r[-1]\n    )\n)\nparsed_rdd.take(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Spark analysis\n\nOne way to analyze data in Spark is to call methods on a dataframe."}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nsqlContext = SQLContext(sc)\ndf = sqlContext.createDataFrame(parsed_rdd)\nconnections_by_protocol = df.groupBy('protocol_type').count().orderBy('count', ascending=False)\nconnections_by_protocol.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Another way is to use Spark SQL"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\ndf.registerTempTable(\"connections\")\nattack_stats = sqlContext.sql(\"\"\"\n                           SELECT \n                             protocol_type, \n                             CASE label\n                               WHEN 'normal.' THEN 'no attack'\n                               ELSE 'attack'\n                             END AS state,\n                             COUNT(*) as total_freq,\n                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n                             ROUND(AVG(duration), 2) as mean_duration,\n                             SUM(num_failed_logins) as total_failed_logins,\n                             SUM(num_compromised) as total_compromised,\n                             SUM(num_file_creations) as total_file_creations,\n                             SUM(su_attempted) as total_root_attempts,\n                             SUM(num_root) as total_root_acceses\n                           FROM connections\n                           GROUP BY protocol_type, state\n                           ORDER BY 3 DESC\n                           \"\"\")\nattack_stats.show()"}, {"cell_type": "code", "execution_count": 6, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nax = attack_stats.toPandas().plot.bar(x='protocol_type', subplots=True, figsize=(10,25))"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nax[0].get_figure().savefig('report.png');"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nimport google.cloud.storage as gcs\nbucket = gcs.Client().get_bucket(BUCKET)\nfor blob in bucket.list_blobs(prefix='sparktodp/'):\n    blob.delete()\nbucket.blob('sparktodp/report.png').upload_from_filename('report.png')"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nconnections_by_protocol.write.format(\"csv\").mode(\"overwrite\").save(\n    \"gs://{}/sparktodp/connections_by_protocol\".format(BUCKET))"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing to qwiklabs-gcp-01-cd84fb191978\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n20/08/12 04:44:48 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n+-------------+------+\n|protocol_type| count|\n+-------------+------+\n|         icmp|283602|\n|          tcp|190065|\n|          udp| 20354|\n+-------------+------+\n\n+-------------+---------+----------+--------------+--------------+-------------+-------------------+-----------------+--------------------+-------------------+------------------+\n|protocol_type|    state|total_freq|mean_src_bytes|mean_dst_bytes|mean_duration|total_failed_logins|total_compromised|total_file_creations|total_root_attempts|total_root_acceses|\n+-------------+---------+----------+--------------+--------------+-------------+-------------------+-----------------+--------------------+-------------------+------------------+\n|         icmp|   attack|    282314|        932.14|           0.0|          0.0|                  0|                0|                   0|                0.0|                 0|\n|          tcp|   attack|    113252|       9880.38|        881.41|        23.19|                 57|             2269|                  76|                1.0|               152|\n|          tcp|no attack|     76813|       1439.31|       4263.97|        11.08|                 18|             2776|                 459|               17.0|              5456|\n|          udp|no attack|     19177|         98.01|         89.89|      1054.63|                  0|                0|                   0|                0.0|                 0|\n|         icmp|no attack|      1288|         91.47|           0.0|          0.0|                  0|                0|                   0|                0.0|                 0|\n|          udp|   attack|      1177|          27.5|          0.23|          0.0|                  0|                0|                   0|                0.0|                 0|\n+-------------+---------+----------+--------------+--------------+-------------+-------------------+-----------------+--------------------+-------------------+------------------+\n\n"}], "source": "BUCKET_list = !gcloud info --format='value(config.project)'\nBUCKET=BUCKET_list[0]\nprint('Writing to {}'.format(BUCKET))\n!/opt/conda/anaconda/bin/python spark_analysis.py --bucket=$BUCKET"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://qwiklabs-gcp-01-cd84fb191978/sparktodp/\ngs://qwiklabs-gcp-01-cd84fb191978/sparktodp/connections_by_protocol/\ngs://qwiklabs-gcp-01-cd84fb191978/sparktodp/connections_by_protocol/_SUCCESS\ngs://qwiklabs-gcp-01-cd84fb191978/sparktodp/connections_by_protocol/part-00000-c2227c49-fadc-40e1-baf7-ab8b3127c50f-c000.csv\ngs://qwiklabs-gcp-01-cd84fb191978/sparktodp/connections_by_protocol/part-00001-c2227c49-fadc-40e1-baf7-ab8b3127c50f-c000.csv\ngs://qwiklabs-gcp-01-cd84fb191978/sparktodp/connections_by_protocol/part-00002-c2227c49-fadc-40e1-baf7-ab8b3127c50f-c000.csv\ngs://qwiklabs-gcp-01-cd84fb191978/sparktodp/report.png\n"}], "source": "!gsutil ls gs://$BUCKET/sparktodp/**"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Copying file://spark_analysis.py [Content-Type=text/x-python]...\n/ [1 files][  2.8 KiB/  2.8 KiB]                                                \nOperation completed over 1 objects/2.8 KiB.                                      \n"}], "source": "!gsutil cp spark_analysis.py gs://$BUCKET/sparktodp/spark_analysis.py"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}